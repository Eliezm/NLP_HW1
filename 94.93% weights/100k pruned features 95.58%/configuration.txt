K = 13
threshold = 1
lam = 2

C:\Users\eliez\AppData\Local\Programs\Python\Python311\python.exe "C:\Users\eliez\Desktop\חומר לימודי\אביב 2025\שיטות בעיבוד שפה טבעית - 097215\Homework\HW1\HW1\NLP_HW1\main.py" 
[main] No existing weights found—training from scratch.
Keeping 114 words as ‘common’ (>100 occurrences).
you have 488200 features!
488200
f100 2394
f101 4479
f102 6777
f103 3449
f104 846
f105 44
f106 3793
f107 3982
f108 5
f109 1
f110 35
f111 33
f112 15
f113 1
f114 33
f115 28
f116 26
f117 15
f132 123
f_char3 8710
f_char4 7949
f_wordbigram_prev 3761
f_wordbigram_next 3889
f_tagshape 1083
f138 123
f141 126
f142 36
f200 0
f201 0
f202 0
f203 0
f204 0
f300 45
f304 11145
f305 12703
f306 11351
f307 12694
f308 0
f309 81
f310 52
f311 85
f143 88
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =       100000     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  1.30847D+06    |proj g|=  2.57583D+04
 This problem is unconstrained.

At iterate   10    f=  2.52459D+05    |proj g|=  1.18808D+03

At iterate   20    f=  1.43740D+05    |proj g|=  3.61351D+02

At iterate   30    f=  1.04098D+05    |proj g|=  4.09173D+02

At iterate   40    f=  7.20620D+04    |proj g|=  3.70893D+02

At iterate   50    f=  4.68800D+04    |proj g|=  1.06536D+03

At iterate   60    f=  3.07878D+04    |proj g|=  2.57203D+02

At iterate   70    f=  2.16162D+04    |proj g|=  1.40453D+02

At iterate   80    f=  1.74477D+04    |proj g|=  1.27339D+02

At iterate   90    f=  1.55134D+04    |proj g|=  1.28754D+02

At iterate  100    f=  1.43905D+04    |proj g|=  2.08067D+02

At iterate  110    f=  1.38076D+04    |proj g|=  5.20074D+01

At iterate  120    f=  1.35491D+04    |proj g|=  2.42606D+01

At iterate  130    f=  1.34302D+04    |proj g|=  2.75316D+01

At iterate  140    f=  1.33678D+04    |proj g|=  2.15694D+01

At iterate  150    f=  1.33350D+04    |proj g|=  1.95217D+01

At iterate  160    f=  1.33201D+04    |proj g|=  1.85638D+01

At iterate  170    f=  1.33115D+04    |proj g|=  4.05836D+00

At iterate  180    f=  1.33071D+04    |proj g|=  3.91969D+00

At iterate  190    f=  1.33049D+04    |proj g|=  4.05969D+00

At iterate  200    f=  1.33036D+04    |proj g|=  3.39907D+00

At iterate  210    f=  1.33029D+04    |proj g|=  3.66969D+00

At iterate  220    f=  1.33025D+04    |proj g|=  2.03706D+00

At iterate  230    f=  1.33023D+04    |proj g|=  7.36286D-01

At iterate  240    f=  1.33022D+04    |proj g|=  1.02145D+00

At iterate  250    f=  1.33021D+04    |proj g|=  5.55746D-01

At iterate  260    f=  1.33021D+04    |proj g|=  4.53227D-01

At iterate  270    f=  1.33021D+04    |proj g|=  3.46605D-01

At iterate  280    f=  1.33021D+04    |proj g|=  1.96326D-01

At iterate  290    f=  1.33021D+04    |proj g|=  2.01635D-01

At iterate  300    f=  1.33021D+04    |proj g|=  2.85395D-01

At iterate  310    f=  1.33021D+04    |proj g|=  7.90811D-02

At iterate  320    f=  1.33021D+04    |proj g|=  1.02474D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****    324    345      1     0     0   4.961D-02   1.330D+04
  F =   13302.071803476425     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
[main] Tagging test set…
100%|██████████| 1000/1000 [06:47<00:00,  2.45it/s]
[main] Predictions written to predictions.wtag
[main] Evaluating on held-out test1.wtag…
Word-level accuracy: 95.58%

Process finished with exit code 0
